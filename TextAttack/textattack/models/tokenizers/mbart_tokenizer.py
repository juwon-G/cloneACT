"""
mbart Tokenizer
---------------------------------------------------------------------

"""

import transformers


class MbartTokenizer:
    """Uses the Marian NMT tokenizer to convert an input for processing.

    
    * english_to_german: translate English to German
    * english_to_french: translate English to French
    """

    def __init__(self, mode="english_to_french"):
        self.tokenization_prefix = ""
        source_lang = "en"
        if mode == "english_to_german":
            # self.tokenization_prefix = "translate English to German: "
            target_lang = "de"
        elif mode == "english_to_french":
            # self.tokenization_prefix = "translate English to French: "
            target_lang = "fr"
        # elif mode == "english_to_romanian":
            # self.tokenization_prefix = "translate English to Romanian: "
        # elif mode == "summarization":
        #     self.tokenization_prefix = "summarize: "
        # else:
        #     raise ValueError(f"Invalid t5 tokenizer mode {mode}.")

        # self.tokenizer = transformers.AutoTokenizer.from_pretrained(
        #     "t5-base", use_fast=True
        # )
        # self.model_max_length = max_length

        name = 'facebook/mbart-large-50-one-to-many-mmt'
        self.tokenizer = transformers.MBart50TokenizerFast.from_pretrained(name)
        dict_lang = {"en":"en_XX", "de":"de_DE", "fr":"fr_XX", "zh":"zh_CN", "cs":"cs_CZ", "ru":"ru_RU"}
        self.tokenizer.src_lang = dict_lang[source_lang]
        self.tokenizer.tgt_lang = dict_lang[target_lang]
        self.tokenizer.model_max_length = 512
        


    def __call__(self, text, *args, **kwargs):
        """
        Args:
            text (:obj:`str`, :obj:`List[str]`):
                    The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings.
        """
        assert isinstance(text, str) or (
            isinstance(text, (list, tuple))
            and (len(text) == 0 or isinstance(text[0], str))
        ), "`text` must be a string or a list of strings."
        if isinstance(text, str):
            text = self.tokenization_prefix + text
        else:
            for i in range(len(text)):
                text[i] = self.tokenization_prefix + text[i]
        return self.tokenizer(text, *args, **kwargs)

    def decode(self, ids):
        """Converts IDs (typically generated by the model) back to a string."""
        return self.tokenizer.decode(ids,skip_special_tokens=True)
