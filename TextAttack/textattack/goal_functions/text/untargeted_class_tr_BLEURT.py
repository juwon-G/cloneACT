"""
Goal Function for Attempts to minimize the BLEU score
-------------------------------------------------------


"""

import functools

import nltk

import textattack

from datasets import load_metric

from .text_to_text_goal_function import TextToTextGoalFunction
import time

import torch
import numpy as np
from bleurt_pytorch import BleurtConfig, BleurtForSequenceClassification, BleurtTokenizer


class UntargetedClassTrBLEURT(TextToTextGoalFunction):
    """Attempts to change the class of the translation generated by the NMT model by using a pretrained classifier.
    Args:
        target_max_score (float): If set, goal is to reduce model output to
            below this score. Otherwise, goal is to change the overall predicted
            class.
    """

    EPS = 1e-10

    config = BleurtConfig.from_pretrained('lucadiliello/BLEURT-20')
    model_bleurt = BleurtForSequenceClassification.from_pretrained('lucadiliello/BLEURT-20')
    tokenizer_bleurt = BleurtTokenizer.from_pretrained('lucadiliello/BLEURT-20')

    model_bleurt.eval()

    # bleu = load_metric("bleurt", 'BLEURT-20-D12', module_type="metric")
    
    # alpha = 0.5

    def __init__(self, *args, ground_max_score=None, max_min_score=None, dif_score=None, alpha=1, thr=1, num_class=1, cw=False, cl=False, **kwargs):
        self.ground_max_score = ground_max_score
        self.max_min_score = max_min_score
        self.dif_score = dif_score
        self.alpha = alpha
        self.thr = thr
        self.attack = False
        self.attack1 = False
        self.num_class = num_class
        self.cw = cw
        self.cl = cl
        super().__init__(*args, **kwargs)
        

    def clear_cache(self):
        if self.use_cache:
            self._call_model_cache.clear()
        get_bleu.cache_clear()

    def _is_goal_complete(self, model_output, _):
        

        _ = self._get_score(model_output, _)

        if self.num_class==1:
            logits = model_output[1]
            max_index = logits.argmax()

                
            if max_index!=self.ground_truth_output:
                if self.ground_max_score:
                    return logits[self.ground_truth_output] < self.ground_max_score and 1-self.tr_score <= self.thr
                elif self.max_min_score:
                    return logits[max_index] > self.max_min_score and 1-self.tr_score <= self.thr
                elif self.dif_score:
                    dif = logits[max_index] - logits[self.ground_truth_output]
                    return dif > self.dif_score and 1-self.tr_score <= self.thr
                elif self.cl:
                    return 1-self.tr_score > self.thr
                else:
                    return 1-self.tr_score <= self.thr
            else:
                return False
            
        
        elif self.num_class==2:

            logits = model_output[1]
            logits1 = model_output[2]

            max_index = logits.argmax()
            max_index1 = logits1.argmax()

      
            if max_index!=self.ground_truth_output and max_index1!=self.ground_truth_output:
                if self.ground_max_score:
                    class_out = logits[self.ground_truth_output] < self.ground_max_score 
                    class1_out = logits1[self.ground_truth_output] < self.ground_max_score 
                    return class_out and class1_out and 1-self.tr_score <= self.thr
                elif self.max_min_score:
                    class_out = logits[max_index] > self.max_min_score 
                    class1_out = logits1[max_index1] > self.max_min_score 
                    return class_out and class1_out and 1-self.tr_score <= self.thr
                elif self.dif_score:
                    dif = logits[max_index] - logits[self.ground_truth_output]
                    dif1 = logits1[max_index1] - logits1[self.ground_truth_output]
                    return dif > self.dif_score and dif1 > self.dif_score and 1-self.tr_score <= self.thr
                else:
                    return 1-self.tr_score <= self.thr
            else:
                return False

            
                
        elif self.num_class==0:
            return 1-self.tr_score <= self.thr


    def _get_score(self, model_output, _):

        translation = model_output[0]
        
        if self.iter==0:
            self.initial_tr = translation
            self.initial_bleu = 100
            bleu_score = 100
            self.attack = False
            self.attack1 = False
        else:
            bleu_score = get_bleu(translation, self.initial_tr, self.model_bleurt, self.tokenizer_bleurt)
        
        self.iter+=1

        
        if self.initial_bleu==0: 
            tr_score = 1
        else:
            if bleu_score/self.initial_bleu>1:
                tr_score = 0
            else:
                tr_score = 1 - bleu_score/self.initial_bleu


        self.tr_score = tr_score
        logits = model_output[1]
        

        class_score =  - logits[self.ground_truth_output]

        if self.cw:
            logit_sorted = logits.topk(2)
            max_logit = logit_sorted.values[logit_sorted.indices!=self.ground_truth_output][0]

            class_score =  - (logits[self.ground_truth_output] - max_logit)

        if self.num_class==1:
            return class_score + self.alpha * tr_score
        
        elif self.num_class==0:
            return tr_score
        
        elif self.num_class==2:
            logits1 = model_output[2]
            class_score1 =  - logits1[self.ground_truth_output]
            return (class_score+class_score1)/2 + self.alpha * tr_score
        



@functools.lru_cache(maxsize=2**12)
def get_bleu(a, b, model, tokenizer):
    # bleu = load_metric("sacrebleu")
    # bleu_score = bleu.compute(predictions=[a], references=[[b]])['score']
    inputs = tokenizer([b], [a], padding='longest', return_tensors='pt')
    bleu_score = model(**inputs).logits.flatten().tolist()[0]*100
    return bleu_score
